LLM_CONFIG = {
    "provider": "together",   # together | openai
    "model": "meta-llama/Llama-3-8b-instruct",
    "temperature": 0,
    "max_tokens": 1024,
    "timeout": 30
}
